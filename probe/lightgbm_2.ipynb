{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7cf6aa0-eb13-419f-8244-7f03e4602192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import ndcg_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "train_df = pd.read_csv('train/train_df.csv')\n",
    "test_df = pd.read_csv('test/test_df.csv')\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cd977dcd-a707-439e-9de6-257260b012fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.drop(['search_id', 'target'], axis=1)\n",
    "y_train = train_df['target']\n",
    "X_test = test_df.drop(['search_id', 'target'], axis=1)\n",
    "y_test = test_df['target']\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f209130c-1df2-4b54-9d58-26c205697683",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Устанавливаем sampling_strategy так, чтобы минорных примеров было в 5 раз меньше мажорных\n",
    "ros = RandomOverSampler(sampling_strategy=0.2, random_state=0)\n",
    "\n",
    "# Применяем увеличение выборки к обучающим данным\n",
    "# X_train, y_train = ros.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4aab0e08-5c74-4e7b-b015-3730d6e30ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate(model, testing_set_x, testing_set_y):\n",
    "    # Получение предсказанных вероятностей для положительного класса\n",
    "    predictions_proba = model.predict(testing_set_x)\n",
    "    \n",
    "    # Вычисление метрик\n",
    "    accuracy = accuracy_score(testing_set_y, predictions_proba >= 0.5)\n",
    "    roc_auc = roc_auc_score(testing_set_y, predictions_proba)\n",
    "    precision = precision_score(testing_set_y, predictions_proba >= 0.5)\n",
    "    recall = recall_score(testing_set_y, predictions_proba >= 0.5)\n",
    "    pr_auc = average_precision_score(testing_set_y, predictions_proba)\n",
    "    \n",
    "    # Формирование результата\n",
    "    result = pd.DataFrame([[accuracy, precision, recall, roc_auc, pr_auc]], columns=['Accuracy', 'Precision', 'Recall', 'ROC_auc', 'PR_auc'])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b7afeb77-26df-4e31-88ae-b7b51a2c8a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's auc: 0.747433\n",
      "{'valid_0': OrderedDict([('auc', [0.7384221916191226, 0.7474326185323628, 0.742799527837891, 0.7349596694865237, 0.7234015345268542, 0.7178339563250049, 0.7206177454259296, 0.7118433995671848, 0.7190537084398977, 0.7137615581349597, 0.714027149321267, 0.7141943734015346])])}\n",
      "   Accuracy  Precision    Recall   ROC_auc   PR_auc\n",
      "0   0.95291   0.193548  0.352941  0.747433  0.13232\n"
     ]
    }
   ],
   "source": [
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',  # Используем AUC как основную метрику\n",
    "    'verbose': -1,\n",
    "    'scale_pos_weight': 50,\n",
    "}\n",
    "\n",
    "evals_result = {}  # Словарь для сохранения результатов\n",
    "\n",
    "bst = lgb.train(params, train_data, valid_sets=[test_data], callbacks=[lgb.record_evaluation(evals_result), lgb.early_stopping(stopping_rounds=10)])\n",
    "print(evals_result)\n",
    "# Вызов функции evaluate для вычисления метрик на тестовом наборе данных\n",
    "results = evaluate(bst, X_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "73b77b93-8012-4da1-a591-dbef80c609b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bst.predict(X_test, num_iteration=bst.best_iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6552c021-a8ac-4f85-8961-d6044b41b09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG Score: 0.5354627311464872\n"
     ]
    }
   ],
   "source": [
    "# Мы могли бы группировать данные по search_id и рассчитывать NDCG для каждой группы,\n",
    "# но поскольку задача стоит рассчитать NDCG для всех документов, мы рассчитаем её глобально.\n",
    "ndcg_score = ndcg_score([y_test], [y_pred], k=None)\n",
    "print(f'NDCG Score: {ndcg_score}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
